{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1dPsOc4xamc0JoeAwV5CWU9gkke6LWqcC","authorship_tag":"ABX9TyMjH4UpsHQsqpn4/xXCPCS+"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8075521,"sourceType":"datasetVersion","datasetId":4765692}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Problem Statement:\n### This assignment aims to utilize Long Short-Term Memory (LSTM) algorithms to perform next word prediction on a given dataset. Next word prediction involves predicting the next word in a sequence of text based on the preceding words. By implementing LSTM (Long Short-Term Memory) networks, to learn how to process sequential data and capture long-term dependencies for predicting the next word in a sentence.","metadata":{"id":"CY_piZHAU6nA"}},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install tensorflow","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:50:55.376820Z","iopub.execute_input":"2024-04-15T20:50:55.377064Z","iopub.status.idle":"2024-04-15T20:51:01.309666Z","shell.execute_reply.started":"2024-04-15T20:50:55.377041Z","shell.execute_reply":"2024-04-15T20:51:01.308583Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow) (65.5.1)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.15.2)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nCollecting keras<2.16,>=2.15.0\n  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.62.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nRequirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/site-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.36.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow) (24.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow) (4.10.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.29.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.1.1\n    Uninstalling keras-3.1.1:\n      Successfully uninstalled keras-3.1.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### The code provided bellow : Loads the text file and reads its content. Displaying the first 1000 characters can be helpful for verifying if the file is loaded correctly.","metadata":{}},{"cell_type":"code","source":"# Specify the path to your text file\nfile = open('/kaggle/input/lstm-data/LSTM DATA.txt', encoding='utf8')\n\n#This line of code opens the specified text file \n# The encoding as UTF-8, which is commonly used for handling text files containing a variety of characters from different languages","metadata":{"executionInfo":{"elapsed":71,"status":"ok","timestamp":1712654575239,"user":{"displayName":"Debasish sahoo","userId":"15668380936846734359"},"user_tz":-330},"id":"MvS6hFclI2L5","outputId":"ebffdbba-e84e-4d61-ffcf-07c1bf9c1e83","execution":{"iopub.status.busy":"2024-04-15T20:51:09.158927Z","iopub.execute_input":"2024-04-15T20:51:09.159498Z","iopub.status.idle":"2024-04-15T20:51:09.167030Z","shell.execute_reply.started":"2024-04-15T20:51:09.159460Z","shell.execute_reply":"2024-04-15T20:51:09.166396Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"lines = file.readlines()\n\n# Close the files\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:09.587717Z","iopub.execute_input":"2024-04-15T20:51:09.588048Z","iopub.status.idle":"2024-04-15T20:51:09.599628Z","shell.execute_reply.started":"2024-04-15T20:51:09.588019Z","shell.execute_reply":"2024-04-15T20:51:09.598945Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"lines [:5]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:09.917535Z","iopub.execute_input":"2024-04-15T20:51:09.917807Z","iopub.status.idle":"2024-04-15T20:51:09.925052Z","shell.execute_reply.started":"2024-04-15T20:51:09.917781Z","shell.execute_reply":"2024-04-15T20:51:09.924447Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['\\ufeffThe Project Gutenberg eBook of Pride and Prejudice\\n',\n '    \\n',\n 'This ebook is for the use of anyone anywhere in the United States and\\n',\n 'most other parts of the world at no cost and with almost no restrictions\\n',\n 'whatsoever. You may copy it, give it away or re-use it under the terms\\n']"},"metadata":{}}]},{"cell_type":"code","source":"# Join the lines into a single string\ndf = ' '.join(lines)\n\n# Step 1: Removing unnecessary information\nstart_index = df.find(\"The Project Gutenberg eBook of Pride and Prejudice\")\ndf = df[start_index:]\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:10.147144Z","iopub.execute_input":"2024-04-15T20:51:10.147366Z","iopub.status.idle":"2024-04-15T20:51:10.154178Z","shell.execute_reply.started":"2024-04-15T20:51:10.147344Z","shell.execute_reply":"2024-04-15T20:51:10.153600Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Remove unwanted characters\ndf = df.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“', '').replace('”', '')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:11.917768Z","iopub.execute_input":"2024-04-15T20:51:11.918117Z","iopub.status.idle":"2024-04-15T20:51:11.927397Z","shell.execute_reply.started":"2024-04-15T20:51:11.918090Z","shell.execute_reply":"2024-04-15T20:51:11.926797Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df[:500]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:12.177260Z","iopub.execute_input":"2024-04-15T20:51:12.177646Z","iopub.status.idle":"2024-04-15T20:51:12.181520Z","shell.execute_reply.started":"2024-04-15T20:51:12.177612Z","shell.execute_reply":"2024-04-15T20:51:12.180964Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'The Project Gutenberg eBook of Pride and Prejudice      This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.  '"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Preprocessing the text data for further analysis. Let me break down what each step does:\n\n   #### 1) Removing unnecessary information: This step finds the index of a specific string (\"The Project Gutenberg eBook of Pride and Prejudice\") and removes everything before it. This could be useful if you want to remove any metadata or headers from your text.\n\n   #### 2) Tokenizing the text into words: This splits the text into individual words based on whitespace. This step is essential for text processing tasks.\n\n  ####  3) Lowercasing the text: Converting all words to lowercase ensures that the model treats words like \"Word\" and \"word\" as the same, which helps in avoiding duplication.\n\n  ####  4) Removing punctuation and special characters: This step uses regular expressions to remove any characters that are not alphanumeric (letters or numbers). It helps clean up the text and standardize it for analysis.\n\n#### Finally, the processed text is joined back together into a single string. This preprocessed text can now be used for tasks like next-word prediction. ","metadata":{}},{"cell_type":"code","source":" # import the libraries 're' for regular expressions \nimport re\n\n# Step 3: Lowercasing the text\ndf = df.lower() \n\n# Additional cleaning using regex\ndf = re.sub(r'\\ufeff', '', df) \n\ndf = re.sub(r'[“”\"‘’]', '', df)\n","metadata":{"executionInfo":{"elapsed":50,"status":"ok","timestamp":1712654575241,"user":{"displayName":"Debasish sahoo","userId":"15668380936846734359"},"user_tz":-330},"id":"2QiZY4AjJi25","execution":{"iopub.status.busy":"2024-04-15T20:51:13.192310Z","iopub.execute_input":"2024-04-15T20:51:13.192596Z","iopub.status.idle":"2024-04-15T20:51:13.213909Z","shell.execute_reply.started":"2024-04-15T20:51:13.192567Z","shell.execute_reply":"2024-04-15T20:51:13.213244Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Displaying the first 500 characters can be helpful for verifying if the file is processed correctly.¶\ndf[:1000]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:13.446914Z","iopub.execute_input":"2024-04-15T20:51:13.447151Z","iopub.status.idle":"2024-04-15T20:51:13.451137Z","shell.execute_reply.started":"2024-04-15T20:51:13.447128Z","shell.execute_reply":"2024-04-15T20:51:13.450605Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'the project gutenberg ebook of pride and prejudice      this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever. you may copy it, give it away or re-use it under the terms of the project gutenberg license included with this ebook or online at www.gutenberg.org. if you are not located in the united states, you will have to check the laws of the country where you are located before using this ebook.  title: pride and prejudice   author: jane austen  release date: june 1, 1998 [ebook #1342]                 most recently updated: april 14, 2023  language: english  credits: chuck greif and the online distributed proofreading team at http://www.pgdp.net (this file was produced from images available at the internet archive)   *** start of the project gutenberg ebook pride and prejudice ***                                 [illustration:                               george allen                   '"},"metadata":{}}]},{"cell_type":"code","source":"# To find the length of the processed text, you can simply use the len() function. \nlen(df)","metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1712654575242,"user":{"displayName":"Debasish sahoo","userId":"15668380936846734359"},"user_tz":-330},"id":"BVLCrVZya6Gu","outputId":"03b6837f-1cc0-46c9-8ab7-88dcf7cb4a7b","execution":{"iopub.status.busy":"2024-04-15T20:51:13.622113Z","iopub.execute_input":"2024-04-15T20:51:13.622622Z","iopub.status.idle":"2024-04-15T20:51:13.626355Z","shell.execute_reply.started":"2024-04-15T20:51:13.622591Z","shell.execute_reply":"2024-04-15T20:51:13.625828Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"743529"},"metadata":{}}]},{"cell_type":"markdown","source":"  ####  Tokenization: Using the Tokenizer class from Keras,initialize a tokenizer object. Specify the maximum number of words (num_words) and a set of characters to filter out (filters). In this case removing punctuation characters and special characters commonly found in text. Additionally, you set lower=True to convert all words to lowercase during tokenization.\n\n####    Fit Tokenizer on Text: You call the fit_on_texts() method of the tokenizer, passing the preprocessed text as input. This updates the tokenizer's internal vocabulary based on the text provided.\n\n####    Generate Word Indices: After fitting the tokenizer, you retrieve the word index, which is a dictionary mapping words to their respective integer indices.\n\n##### This prepares your text data for further processing, such as converting text sequences to sequences of integers based on these word indices.","metadata":{}},{"cell_type":"code","source":"!pip install optree","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:14.577017Z","iopub.execute_input":"2024-04-15T20:51:14.577325Z","iopub.status.idle":"2024-04-15T20:51:17.896871Z","shell.execute_reply.started":"2024-04-15T20:51:14.577298Z","shell.execute_reply":"2024-04-15T20:51:17.895886Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optree in /usr/local/lib/python3.10/site-packages (0.11.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/site-packages (from optree) (4.10.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Tokenization\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([df])\nword_index = tokenizer.word_index","metadata":{"executionInfo":{"elapsed":4055,"status":"ok","timestamp":1712654579267,"user":{"displayName":"Debasish sahoo","userId":"15668380936846734359"},"user_tz":-330},"id":"5lBXuKw46MBh","outputId":"dbbc1a1d-8d2d-49fc-8a79-9e7aa5ee22af","execution":{"iopub.status.busy":"2024-04-15T20:51:17.898535Z","iopub.execute_input":"2024-04-15T20:51:17.898824Z","iopub.status.idle":"2024-04-15T20:51:21.482946Z","shell.execute_reply.started":"2024-04-15T20:51:17.898796Z","shell.execute_reply":"2024-04-15T20:51:21.481900Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n#### Save the tokenizer as a token.pkl gormat externally, Useing pickle libraries.\npickle.dump(tokenizer,open('token.pkl','wb'))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:21.484507Z","iopub.execute_input":"2024-04-15T20:51:21.485000Z","iopub.status.idle":"2024-04-15T20:51:21.494005Z","shell.execute_reply.started":"2024-04-15T20:51:21.484969Z","shell.execute_reply":"2024-04-15T20:51:21.493134Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"len(word_index) # Check the vocabulary size\n# The length of word_index gives you the number of unique words in your preprocessed text. \n# You can obtain this length using the len() function","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:21.494833Z","iopub.execute_input":"2024-04-15T20:51:21.495137Z","iopub.status.idle":"2024-04-15T20:51:21.522557Z","shell.execute_reply.started":"2024-04-15T20:51:21.495110Z","shell.execute_reply":"2024-04-15T20:51:21.521792Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"7172"},"metadata":{}}]},{"cell_type":"markdown","source":"#### This process effectively creates sequences of fixed length from the tokenized text data, which is often used as input data for models like recurrent neural networks (RNNs) or long short-term memory networks (LSTMs) in tasks like sequence prediction or language modeling.","metadata":{}},{"cell_type":"code","source":"# Convert text sequences to sequences of integers\n\ninput_sequences = [] # This initializes an empty list input_sequences where we will store the sequences of integers.\n\nfor sentence in df.split('. '):  # This loop iterates over each sentence in the text data. \n    # The assumption here is that df contains the text data, and split('. ') is used to split the text into sentences based on the period followed by a space, \n    # which is a common way to denote the end of a sentence. You may need to adjust this based on the structure of your text data.\n    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0] #Inside the loop, each sentence is tokenized into a sequence of integers using the texts_to_sequences method of the tokenizer object. \n    # This method converts each word in the sentence to its corresponding integer index based on the vocabulary learned during tokenization.\n    \n    for i in range(1, len(tokenized_sentence)):\n        input_sequences.append(tokenized_sentence[:i+1])\n        # For each tokenized sentence, this loop generates multiple input-output pairs for training the model. \n        # It starts from the second word (index 1) and iterates up to the length of the tokenized sentence.\n        # At each iteration, it creates a sequence of integers from the beginning of the sentence up to the current word index (i+1). \n        # This sequence represents the input, and the next word after this sequence represents the output. \n        # These input-output pairs are then appended to the input_sequences list.","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:22.842100Z","iopub.execute_input":"2024-04-15T20:51:22.842452Z","iopub.status.idle":"2024-04-15T20:51:23.183128Z","shell.execute_reply.started":"2024-04-15T20:51:22.842421Z","shell.execute_reply":"2024-04-15T20:51:23.182311Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### After running this code, input_sequences will contain sequences of integers representing input-output pairs for training the model. Each sequence consists of an increasing number of words from the beginning of a sentence, and the final element of each sequence is the next word in the sentence. These sequences can be used to train an LSTM or similar model for next word prediction.","metadata":{}},{"cell_type":"code","source":"input_sequences [0:15]","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:23.217110Z","iopub.execute_input":"2024-04-15T20:51:23.217371Z","iopub.status.idle":"2024-04-15T20:51:23.223894Z","shell.execute_reply.started":"2024-04-15T20:51:23.217338Z","shell.execute_reply":"2024-04-15T20:51:23.223179Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[[1, 182],\n [1, 182, 391],\n [1, 182, 391, 1005],\n [1, 182, 391, 1005, 3],\n [1, 182, 391, 1005, 3, 299],\n [1, 182, 391, 1005, 3, 299, 4],\n [1, 182, 391, 1005, 3, 299, 4, 951],\n [1, 182, 391, 1005, 3, 299, 4, 951, 41],\n [1, 182, 391, 1005, 3, 299, 4, 951, 41, 1005],\n [1, 182, 391, 1005, 3, 299, 4, 951, 41, 1005, 23],\n [1, 182, 391, 1005, 3, 299, 4, 951, 41, 1005, 23, 21],\n [1, 182, 391, 1005, 3, 299, 4, 951, 41, 1005, 23, 21, 1],\n [1, 182, 391, 1005, 3, 299, 4, 951, 41, 1005, 23, 21, 1, 508],\n [1, 182, 391, 1005, 3, 299, 4, 951, 41, 1005, 23, 21, 1, 508, 3],\n [1, 182, 391, 1005, 3, 299, 4, 951, 41, 1005, 23, 21, 1, 508, 3, 549]]"},"metadata":{}}]},{"cell_type":"code","source":"max_len = max([len(x) for x in input_sequences])\nmax_len\n\n# This code calculates the maximum length of the input sequences.\n# max_len: This is the maximum length that you want the sequences to be padded or truncated to. \n# All sequences longer than maxlen will be truncated, and sequences shorter than maxlen will be padded.","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:25.017552Z","iopub.execute_input":"2024-04-15T20:51:25.017894Z","iopub.status.idle":"2024-04-15T20:51:25.036187Z","shell.execute_reply.started":"2024-04-15T20:51:25.017866Z","shell.execute_reply":"2024-04-15T20:51:25.035572Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"152"},"metadata":{}}]},{"cell_type":"code","source":"# Pad the sequences This code snippet pads the input sequences to ensure that they all have the same length.\npadded_input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='pre')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:25.212353Z","iopub.execute_input":"2024-04-15T20:51:25.212631Z","iopub.status.idle":"2024-04-15T20:51:25.549011Z","shell.execute_reply.started":"2024-04-15T20:51:25.212590Z","shell.execute_reply":"2024-04-15T20:51:25.548256Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"padded_input_sequences","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:27.242412Z","iopub.execute_input":"2024-04-15T20:51:27.243267Z","iopub.status.idle":"2024-04-15T20:51:27.248580Z","shell.execute_reply.started":"2024-04-15T20:51:27.243222Z","shell.execute_reply":"2024-04-15T20:51:27.247886Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([[   0,    0,    0, ...,    0,    1,  182],\n       [   0,    0,    0, ...,    1,  182,  391],\n       [   0,    0,    0, ...,  182,  391, 1005],\n       ...,\n       [   0,    0,    0, ...,    2,  230,  126],\n       [   0,    0,    0, ...,  230,  126,  524],\n       [   0,    0,    0, ...,  126,  524, 1753]], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"len(padded_input_sequences)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:27.458092Z","iopub.execute_input":"2024-04-15T20:51:27.459042Z","iopub.status.idle":"2024-04-15T20:51:27.465563Z","shell.execute_reply.started":"2024-04-15T20:51:27.458973Z","shell.execute_reply":"2024-04-15T20:51:27.464620Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"124648"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n# Create predictors and labels\nX = np.array([x[:-1] for x in padded_input_sequences])\ny = np.array([x[-1] for x in padded_input_sequences])\n\n# Print the shapes of X and y\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y: {y.shape}\")","metadata":{"executionInfo":{"elapsed":782,"status":"ok","timestamp":1712654580030,"user":{"displayName":"Debasish sahoo","userId":"15668380936846734359"},"user_tz":-330},"id":"uwBzJyuHOXGr","outputId":"b3034956-c755-4ab2-f8ef-9011b6e252e3","execution":{"iopub.status.busy":"2024-04-15T20:51:27.672588Z","iopub.execute_input":"2024-04-15T20:51:27.672978Z","iopub.status.idle":"2024-04-15T20:51:27.830881Z","shell.execute_reply.started":"2024-04-15T20:51:27.672946Z","shell.execute_reply":"2024-04-15T20:51:27.830198Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Shape of X: (124648, 151)\nShape of y: (124648,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n\n# Convert y to one-hot encoding\nnum_words = len(word_index) + 1\ny_one_hot = tf.keras.utils.to_categorical(y, num_classes=num_words)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:29.062421Z","iopub.execute_input":"2024-04-15T20:51:29.063099Z","iopub.status.idle":"2024-04-15T20:51:29.415425Z","shell.execute_reply.started":"2024-04-15T20:51:29.063059Z","shell.execute_reply":"2024-04-15T20:51:29.414632Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### In summary, using one-hot encoding for categorical labels ensures compatibility with loss functions, facilitates meaningful comparison between model predictions and ground truth labels, and helps in stable training and effective learning of neural network models, especially for classification tasks.","metadata":{}},{"cell_type":"code","source":"y_one_hot.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:29.497191Z","iopub.execute_input":"2024-04-15T20:51:29.497450Z","iopub.status.idle":"2024-04-15T20:51:29.501955Z","shell.execute_reply.started":"2024-04-15T20:51:29.497426Z","shell.execute_reply":"2024-04-15T20:51:29.501201Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(124648, 7173)"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM,BatchNormalization","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:29.737127Z","iopub.execute_input":"2024-04-15T20:51:29.737360Z","iopub.status.idle":"2024-04-15T20:51:29.741753Z","shell.execute_reply.started":"2024-04-15T20:51:29.737337Z","shell.execute_reply":"2024-04-15T20:51:29.741159Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dropout, BatchNormalization, Dense\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=15000, output_dim=100, input_shape=(151,)))\nmodel.add(LSTM(256, return_sequences=True))\nmodel.add(Dropout(rate=0.2))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dropout(rate=0.2))\nmodel.add(LSTM(64))\nmodel.add(BatchNormalization())\nmodel.add(Dense(7173, activation='softmax'))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:32.497894Z","iopub.execute_input":"2024-04-15T20:51:32.498212Z","iopub.status.idle":"2024-04-15T20:51:35.743718Z","shell.execute_reply.started":"2024-04-15T20:51:32.498187Z","shell.execute_reply":"2024-04-15T20:51:35.742894Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"2024-04-15 20:51:34.977364: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.977477: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.977553: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.977639: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.977707: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.977907: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.977983: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.978076: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.978168: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.978252: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.978459: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.978547: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.978632: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.978721: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.978806: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979002: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979084: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979187: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979270: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979345: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979549: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979634: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979709: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979783: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.979878: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.980122: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.980207: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.980298: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.980375: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.980458: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.980709: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.980814: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.980914: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.980994: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.981082: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.981325: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.981475: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.981567: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.981644: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n2024-04-15 20:51:34.981727: E external/local_xla/xla/stream_executor/stream_executor_internal.h:177] SetPriority unimplemented for this stream.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:40.297701Z","iopub.execute_input":"2024-04-15T20:51:40.298062Z","iopub.status.idle":"2024-04-15T20:51:40.318245Z","shell.execute_reply.started":"2024-04-15T20:51:40.298034Z","shell.execute_reply":"2024-04-15T20:51:40.317538Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding (Embedding)       (None, 151, 100)          1500000   \n                                                                 \n lstm (LSTM)                 (None, 151, 256)          365568    \n                                                                 \n dropout (Dropout)           (None, 151, 256)          0         \n                                                                 \n lstm_1 (LSTM)               (None, 151, 128)          197120    \n                                                                 \n dropout_1 (Dropout)         (None, 151, 128)          0         \n                                                                 \n lstm_2 (LSTM)               (None, 64)                49408     \n                                                                 \n batch_normalization (Batch  (None, 64)                256       \n Normalization)                                                  \n                                                                 \n dense (Dense)               (None, 7173)              466245    \n                                                                 \n=================================================================\nTotal params: 2578597 (9.84 MB)\nTrainable params: 2578469 (9.84 MB)\nNon-trainable params: 128 (512.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:41.262742Z","iopub.execute_input":"2024-04-15T20:51:41.263084Z","iopub.status.idle":"2024-04-15T20:51:41.281662Z","shell.execute_reply.started":"2024-04-15T20:51:41.263056Z","shell.execute_reply":"2024-04-15T20:51:41.280872Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Train the model\nbatch_size = 64\nepochs = 50\n\nhistory = model.fit(X, y_one_hot, batch_size=batch_size, validation_split=0.2, epochs=epochs)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T20:51:44.737791Z","iopub.execute_input":"2024-04-15T20:51:44.738591Z","iopub.status.idle":"2024-04-15T22:48:54.042557Z","shell.execute_reply.started":"2024-04-15T20:51:44.738554Z","shell.execute_reply":"2024-04-15T22:48:54.040885Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 1/50\n1559/1559 [==============================] - 946s 604ms/step - loss: 6.5053 - accuracy: 0.0663 - val_loss: 5.9738 - val_accuracy: 0.0961\nEpoch 2/50\n1559/1559 [==============================] - 940s 603ms/step - loss: 5.5270 - accuracy: 0.1148 - val_loss: 5.7902 - val_accuracy: 0.1184\nEpoch 3/50\n1559/1559 [==============================] - 938s 602ms/step - loss: 5.1879 - accuracy: 0.1375 - val_loss: 5.6963 - val_accuracy: 0.1256\nEpoch 4/50\n1559/1559 [==============================] - 938s 602ms/step - loss: 4.9399 - accuracy: 0.1544 - val_loss: 5.6902 - val_accuracy: 0.1306\nEpoch 5/50\n1559/1559 [==============================] - 937s 601ms/step - loss: 4.7256 - accuracy: 0.1653 - val_loss: 5.7460 - val_accuracy: 0.1318\nEpoch 6/50\n1559/1559 [==============================] - 938s 602ms/step - loss: 4.5329 - accuracy: 0.1762 - val_loss: 5.8621 - val_accuracy: 0.1349\nEpoch 7/50\n1559/1559 [==============================] - 938s 602ms/step - loss: 4.3478 - accuracy: 0.1879 - val_loss: 5.9609 - val_accuracy: 0.1322\nEpoch 8/50\n 814/1559 [==============>...............] - ETA: 6:47 - loss: 4.0990 - accuracy: 0.2073","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot training history\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T22:48:54.043284Z","iopub.status.idle":"2024-04-15T22:48:54.043629Z","shell.execute_reply.started":"2024-04-15T22:48:54.043453Z","shell.execute_reply":"2024-04-15T22:48:54.043471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('next_word_predictor.h5')","metadata":{"execution":{"iopub.status.busy":"2024-04-15T22:48:54.044405Z","iopub.status.idle":"2024-04-15T22:48:54.044724Z","shell.execute_reply.started":"2024-04-15T22:48:54.044561Z","shell.execute_reply":"2024-04-15T22:48:54.044578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport time\nimport numpy as np\n\ntext = \"the\"\n\nfor i in range(25):\n  # tokenize\n  token_text = tokenizer.texts_to_sequences([text])[0]\n  # padding\n  padded_token_text = pad_sequences([token_text], maxlen=3, padding='pre')\n  # predict\n  predicted_index = np.argmax(model.predict(padded_token_text))\n    \n  for word,index in tokenizer.word_index.items():\n    if index == predicted_index:\n      text = text + \" \" + word\n      print(text)\n      time.sleep(2)\n  ","metadata":{"execution":{"iopub.status.busy":"2024-04-15T22:48:54.045375Z","iopub.status.idle":"2024-04-15T22:48:54.045646Z","shell.execute_reply.started":"2024-04-15T22:48:54.045509Z","shell.execute_reply":"2024-04-15T22:48:54.045523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tuning the Hyperparameters\n","metadata":{}},{"cell_type":"code","source":"!pip install scikeras==0.12.0","metadata":{"execution":{"iopub.status.busy":"2024-04-15T22:48:54.046187Z","iopub.status.idle":"2024-04-15T22:48:54.046446Z","shell.execute_reply.started":"2024-04-15T22:48:54.046316Z","shell.execute_reply":"2024-04-15T22:48:54.046329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom sklearn.model_selection import GridSearchCV\nfrom scikeras.wrappers import KerasClassifier\n\n# Define the LSTM model\n# Define the LSTM model\ndef create_lstm_model():\n    model.add(Embedding(input_dim=15000, output_dim=100, input_shape=(151,)))\n    model.add(LSTM(256, return_sequences=True))\n    model.add(Dropout(rate=0.2))\n    model.add(LSTM(128, return_sequences=True))\n    model.add(Dropout(rate=0.2))\n    model.add(LSTM(64))\n    model.add(BatchNormalization())\n    model.add(Dense(7173, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-04-15T22:48:54.047078Z","iopub.status.idle":"2024-04-15T22:48:54.047341Z","shell.execute_reply.started":"2024-04-15T22:48:54.047212Z","shell.execute_reply":"2024-04-15T22:48:54.047226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create model\nmodel = KerasClassifier(model=create_lstm_model, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T22:48:54.048227Z","iopub.status.idle":"2024-04-15T22:48:54.048503Z","shell.execute_reply.started":"2024-04-15T22:48:54.048365Z","shell.execute_reply":"2024-04-15T22:48:54.048380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define hyperparameters for grid search\nparam_grid = {\n    'optimizer': ['adam', 'rmsprop'],\n    'batch_size': [64, 128, 256],\n    'epochs': [50, 100],\n}\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)","metadata":{"id":"NOS_60Y8DoRE","outputId":"34a3cebb-0107-463b-951b-15a76a505bc7","execution":{"iopub.status.busy":"2024-04-15T22:48:54.049168Z","iopub.status.idle":"2024-04-15T22:48:54.049481Z","shell.execute_reply.started":"2024-04-15T22:48:54.049335Z","shell.execute_reply":"2024-04-15T22:48:54.049351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_result = grid.fit(X,y_one_hot)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T22:48:54.050289Z","iopub.status.idle":"2024-04-15T22:48:54.050607Z","shell.execute_reply.started":"2024-04-15T22:48:54.050449Z","shell.execute_reply":"2024-04-15T22:48:54.050465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T22:48:54.051401Z","iopub.status.idle":"2024-04-15T22:48:54.051730Z","shell.execute_reply.started":"2024-04-15T22:48:54.051556Z","shell.execute_reply":"2024-04-15T22:48:54.051572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"2mnAdwcankRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}